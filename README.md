# syntax-driven-embeddings
Code for the project of the Methods in AI Research (MAIR) course.
# Abstract
A recently emerging approach to natural language processing
is distributional semantics. Distributional semantics are an
application of the notion that a words meaning can be derived
from its context. While largely successful, most established
state-of-the-art models assume the same, coarse way of
determining context; through positional distance. We look
into a different way of determining context, namely by using
dependency trees and show that this approach can potentially
outperform models based on positional distance in defining
various semantic relations.
